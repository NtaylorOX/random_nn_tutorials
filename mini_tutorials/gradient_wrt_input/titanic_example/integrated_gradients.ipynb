{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 => Loss: 0.70\n",
      "Epoch 21/200 => Loss: 0.55\n",
      "Epoch 41/200 => Loss: 0.50\n",
      "Epoch 61/200 => Loss: 0.49\n",
      "Epoch 81/200 => Loss: 0.48\n",
      "Epoch 101/200 => Loss: 0.49\n",
      "Epoch 121/200 => Loss: 0.48\n",
      "Epoch 141/200 => Loss: 0.48\n",
      "Epoch 161/200 => Loss: 0.48\n",
      "Epoch 181/200 => Loss: 0.48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import grad\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "        \n",
    "# Download dataset from: https://biostat.app.vumc.org/wiki/pub/Main/DataSets/titanic3.csv\n",
    "dataset_path = \"../../../../data/titanic3.csv\"\n",
    "titanic_data = pd.read_csv(dataset_path)\n",
    "\n",
    "titanic_data = pd.concat([titanic_data,\n",
    "                          pd.get_dummies(titanic_data['sex']),\n",
    "                          pd.get_dummies(titanic_data['embarked'],prefix=\"embark\"),\n",
    "                          pd.get_dummies(titanic_data['pclass'],prefix=\"class\")], axis=1)\n",
    "titanic_data[\"age\"] = titanic_data[\"age\"].fillna(titanic_data[\"age\"].mean())\n",
    "titanic_data[\"fare\"] = titanic_data[\"fare\"].fillna(titanic_data[\"fare\"].mean())\n",
    "titanic_data = titanic_data.drop(['name','ticket','cabin','boat','body','home.dest','sex','embarked','pclass'], axis=1)\n",
    "\n",
    "# Set random seed for reproducibility.\n",
    "np.random.seed(131254)\n",
    "\n",
    "# Convert features and labels to numpy arrays.\n",
    "labels = titanic_data[\"survived\"].to_numpy()\n",
    "titanic_data = titanic_data.drop(['survived'], axis=1)\n",
    "feature_names = list(titanic_data.columns)\n",
    "data = titanic_data.to_numpy()\n",
    "\n",
    "# Separate training and test sets using \n",
    "train_indices = np.random.choice(len(labels), int(0.7*len(labels)), replace=False)\n",
    "test_indices = list(set(range(len(labels))) - set(train_indices))\n",
    "train_features = data[train_indices]\n",
    "train_labels = labels[train_indices]\n",
    "test_features = data[test_indices]\n",
    "test_labels = labels[test_indices]\n",
    "\n",
    "\n",
    "torch.manual_seed(1)  # Set seed for reproducibility.\n",
    "class TitanicSimpleNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(12, 12)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.linear2 = nn.Linear(12, 8)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.linear3 = nn.Linear(8, 2)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lin1_out = self.linear1(x)\n",
    "        sigmoid_out1 = self.sigmoid1(lin1_out)\n",
    "        sigmoid_out2 = self.sigmoid2(self.linear2(sigmoid_out1))\n",
    "        return self.softmax(self.linear3(sigmoid_out2))\n",
    "        \n",
    "        \n",
    "net = TitanicSimpleNNModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_epochs = 200\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "input_tensor = torch.from_numpy(train_features).type(torch.FloatTensor)\n",
    "label_tensor = torch.from_numpy(train_labels)\n",
    "for epoch in range(num_epochs):    \n",
    "    output = net(input_tensor)\n",
    "    loss = criterion(output, label_tensor)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 20 == 0:\n",
    "        print ('Epoch {}/{} => Loss: {:.2f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "        \n",
    "input_tensor = torch.from_numpy(train_features).type(torch.FloatTensor)\n",
    "test_input_tensor = torch.from_numpy(test_features).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(916, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "idx = 1\n",
    "\n",
    "def preds_and_grads(inputs, model, baselines=None, n_steps=50, target=1, numpy=False):\n",
    "    \"\"\"\n",
    "    function to get predictions and gradients of the output wrt features values\n",
    "    \n",
    "    Args:\n",
    "        inputs : Tensor of inputs\n",
    "        model : pytorch model\n",
    "        baselines : Tensor or None of baselines \n",
    "        n_steps : int number of steps to approximate integrated gradients\n",
    "        target : int target class\n",
    "        numpy : bool if true return numpy, else Tensor\n",
    "    \"\"\"\n",
    "    if inputs.dim() == 2:\n",
    "        inputs = inputs.unsqueeze(0)\n",
    "    \n",
    "    if baselines == None:\n",
    "        baselines = torch.zeros_like(inputs)\n",
    "        \n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"baselines shape: {baselines.shape}\")\n",
    "    \n",
    "    # k/m in the formula\n",
    "    alphas = torch.linspace(0, 1, n_steps).tolist()\n",
    "    \n",
    "    print(f\"alphas: {alphas}\")\n",
    "    \n",
    "    # direct path from baseline to input. shape : ([n_steps, n_features], )\n",
    "    scaled_features = tuple(\n",
    "            torch.cat(\n",
    "                [baseline + alpha * (input - baseline) for alpha in alphas], dim=0\n",
    "            ).requires_grad_()\n",
    "            for input, baseline in zip(inputs, baselines)\n",
    "        )\n",
    "    print(f\"scaled features len shape :{scaled_features} {len(scaled_features)} and {scaled_features[0].shape}\")\n",
    "    # predictions at every step. shape : [n_steps, 1]\n",
    "    preds = model(scaled_features[0])[:, target]\n",
    "    # gradients of predictions wrt input features. shape : [n_steps, n_features]\n",
    "    grads = grad(outputs=torch.unbind(preds), inputs=scaled_features)\n",
    "    print(f\"grads shape is: {grads[0].shape}\")\n",
    "    if numpy:\n",
    "        return preds.detach().numpy(), grads[0].detach().numpy()\n",
    "    return preds, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs shape: torch.Size([1, 1, 12])\n",
      "baselines shape: torch.Size([1, 1, 12])\n",
      "alphas: [0.0, 0.020408162847161293, 0.040816325694322586, 0.06122449040412903, 0.08163265138864517, 0.10204081237316132, 0.12244898080825806, 0.1428571343421936, 0.16326530277729034, 0.18367347121238708, 0.20408162474632263, 0.22448979318141937, 0.2448979616165161, 0.26530611515045166, 0.2857142686843872, 0.30612242221832275, 0.3265306055545807, 0.34693875908851624, 0.36734694242477417, 0.3877550959587097, 0.40816324949264526, 0.4285714030265808, 0.44897958636283875, 0.4693877398967743, 0.4897959232330322, 0.5102040767669678, 0.5306122303009033, 0.5510203838348389, 0.5714285969734192, 0.5918367505073547, 0.6122449040412903, 0.6326530575752258, 0.6530612707138062, 0.6734694242477417, 0.6938775777816772, 0.7142857313156128, 0.7346939444541931, 0.7551020979881287, 0.7755102515220642, 0.7959184050559998, 0.8163264989852905, 0.8367346525192261, 0.8571428060531616, 0.8775509595870972, 0.8979591727256775, 0.918367326259613, 0.9387754797935486, 0.9591836333274841, 0.9795918464660645, 1.0]\n",
      "scaled features len shape :(tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "        [9.5918e-01, 2.0408e-02, 0.0000e+00, 4.6434e+00, 0.0000e+00, 2.0408e-02,\n",
      "         2.0408e-02, 0.0000e+00, 0.0000e+00, 2.0408e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [1.9184e+00, 4.0816e-02, 0.0000e+00, 9.2867e+00, 0.0000e+00, 4.0816e-02,\n",
      "         4.0816e-02, 0.0000e+00, 0.0000e+00, 4.0816e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [2.8776e+00, 6.1224e-02, 0.0000e+00, 1.3930e+01, 0.0000e+00, 6.1224e-02,\n",
      "         6.1224e-02, 0.0000e+00, 0.0000e+00, 6.1224e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [3.8367e+00, 8.1633e-02, 0.0000e+00, 1.8573e+01, 0.0000e+00, 8.1633e-02,\n",
      "         8.1633e-02, 0.0000e+00, 0.0000e+00, 8.1633e-02, 0.0000e+00, 0.0000e+00],\n",
      "        [4.7959e+00, 1.0204e-01, 0.0000e+00, 2.3217e+01, 0.0000e+00, 1.0204e-01,\n",
      "         1.0204e-01, 0.0000e+00, 0.0000e+00, 1.0204e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [5.7551e+00, 1.2245e-01, 0.0000e+00, 2.7860e+01, 0.0000e+00, 1.2245e-01,\n",
      "         1.2245e-01, 0.0000e+00, 0.0000e+00, 1.2245e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [6.7143e+00, 1.4286e-01, 0.0000e+00, 3.2504e+01, 0.0000e+00, 1.4286e-01,\n",
      "         1.4286e-01, 0.0000e+00, 0.0000e+00, 1.4286e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [7.6735e+00, 1.6327e-01, 0.0000e+00, 3.7147e+01, 0.0000e+00, 1.6327e-01,\n",
      "         1.6327e-01, 0.0000e+00, 0.0000e+00, 1.6327e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [8.6327e+00, 1.8367e-01, 0.0000e+00, 4.1790e+01, 0.0000e+00, 1.8367e-01,\n",
      "         1.8367e-01, 0.0000e+00, 0.0000e+00, 1.8367e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [9.5918e+00, 2.0408e-01, 0.0000e+00, 4.6434e+01, 0.0000e+00, 2.0408e-01,\n",
      "         2.0408e-01, 0.0000e+00, 0.0000e+00, 2.0408e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.0551e+01, 2.2449e-01, 0.0000e+00, 5.1077e+01, 0.0000e+00, 2.2449e-01,\n",
      "         2.2449e-01, 0.0000e+00, 0.0000e+00, 2.2449e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.1510e+01, 2.4490e-01, 0.0000e+00, 5.5720e+01, 0.0000e+00, 2.4490e-01,\n",
      "         2.4490e-01, 0.0000e+00, 0.0000e+00, 2.4490e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.2469e+01, 2.6531e-01, 0.0000e+00, 6.0364e+01, 0.0000e+00, 2.6531e-01,\n",
      "         2.6531e-01, 0.0000e+00, 0.0000e+00, 2.6531e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.3429e+01, 2.8571e-01, 0.0000e+00, 6.5007e+01, 0.0000e+00, 2.8571e-01,\n",
      "         2.8571e-01, 0.0000e+00, 0.0000e+00, 2.8571e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.4388e+01, 3.0612e-01, 0.0000e+00, 6.9651e+01, 0.0000e+00, 3.0612e-01,\n",
      "         3.0612e-01, 0.0000e+00, 0.0000e+00, 3.0612e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.5347e+01, 3.2653e-01, 0.0000e+00, 7.4294e+01, 0.0000e+00, 3.2653e-01,\n",
      "         3.2653e-01, 0.0000e+00, 0.0000e+00, 3.2653e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.6306e+01, 3.4694e-01, 0.0000e+00, 7.8937e+01, 0.0000e+00, 3.4694e-01,\n",
      "         3.4694e-01, 0.0000e+00, 0.0000e+00, 3.4694e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.7265e+01, 3.6735e-01, 0.0000e+00, 8.3581e+01, 0.0000e+00, 3.6735e-01,\n",
      "         3.6735e-01, 0.0000e+00, 0.0000e+00, 3.6735e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.8224e+01, 3.8776e-01, 0.0000e+00, 8.8224e+01, 0.0000e+00, 3.8776e-01,\n",
      "         3.8776e-01, 0.0000e+00, 0.0000e+00, 3.8776e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [1.9184e+01, 4.0816e-01, 0.0000e+00, 9.2867e+01, 0.0000e+00, 4.0816e-01,\n",
      "         4.0816e-01, 0.0000e+00, 0.0000e+00, 4.0816e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.0143e+01, 4.2857e-01, 0.0000e+00, 9.7511e+01, 0.0000e+00, 4.2857e-01,\n",
      "         4.2857e-01, 0.0000e+00, 0.0000e+00, 4.2857e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.1102e+01, 4.4898e-01, 0.0000e+00, 1.0215e+02, 0.0000e+00, 4.4898e-01,\n",
      "         4.4898e-01, 0.0000e+00, 0.0000e+00, 4.4898e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.2061e+01, 4.6939e-01, 0.0000e+00, 1.0680e+02, 0.0000e+00, 4.6939e-01,\n",
      "         4.6939e-01, 0.0000e+00, 0.0000e+00, 4.6939e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.3020e+01, 4.8980e-01, 0.0000e+00, 1.1144e+02, 0.0000e+00, 4.8980e-01,\n",
      "         4.8980e-01, 0.0000e+00, 0.0000e+00, 4.8980e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.3980e+01, 5.1020e-01, 0.0000e+00, 1.1608e+02, 0.0000e+00, 5.1020e-01,\n",
      "         5.1020e-01, 0.0000e+00, 0.0000e+00, 5.1020e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.4939e+01, 5.3061e-01, 0.0000e+00, 1.2073e+02, 0.0000e+00, 5.3061e-01,\n",
      "         5.3061e-01, 0.0000e+00, 0.0000e+00, 5.3061e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.5898e+01, 5.5102e-01, 0.0000e+00, 1.2537e+02, 0.0000e+00, 5.5102e-01,\n",
      "         5.5102e-01, 0.0000e+00, 0.0000e+00, 5.5102e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.6857e+01, 5.7143e-01, 0.0000e+00, 1.3001e+02, 0.0000e+00, 5.7143e-01,\n",
      "         5.7143e-01, 0.0000e+00, 0.0000e+00, 5.7143e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.7816e+01, 5.9184e-01, 0.0000e+00, 1.3466e+02, 0.0000e+00, 5.9184e-01,\n",
      "         5.9184e-01, 0.0000e+00, 0.0000e+00, 5.9184e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.8776e+01, 6.1224e-01, 0.0000e+00, 1.3930e+02, 0.0000e+00, 6.1224e-01,\n",
      "         6.1224e-01, 0.0000e+00, 0.0000e+00, 6.1224e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [2.9735e+01, 6.3265e-01, 0.0000e+00, 1.4394e+02, 0.0000e+00, 6.3265e-01,\n",
      "         6.3265e-01, 0.0000e+00, 0.0000e+00, 6.3265e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.0694e+01, 6.5306e-01, 0.0000e+00, 1.4859e+02, 0.0000e+00, 6.5306e-01,\n",
      "         6.5306e-01, 0.0000e+00, 0.0000e+00, 6.5306e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.1653e+01, 6.7347e-01, 0.0000e+00, 1.5323e+02, 0.0000e+00, 6.7347e-01,\n",
      "         6.7347e-01, 0.0000e+00, 0.0000e+00, 6.7347e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.2612e+01, 6.9388e-01, 0.0000e+00, 1.5787e+02, 0.0000e+00, 6.9388e-01,\n",
      "         6.9388e-01, 0.0000e+00, 0.0000e+00, 6.9388e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.3571e+01, 7.1429e-01, 0.0000e+00, 1.6252e+02, 0.0000e+00, 7.1429e-01,\n",
      "         7.1429e-01, 0.0000e+00, 0.0000e+00, 7.1429e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.4531e+01, 7.3469e-01, 0.0000e+00, 1.6716e+02, 0.0000e+00, 7.3469e-01,\n",
      "         7.3469e-01, 0.0000e+00, 0.0000e+00, 7.3469e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.5490e+01, 7.5510e-01, 0.0000e+00, 1.7180e+02, 0.0000e+00, 7.5510e-01,\n",
      "         7.5510e-01, 0.0000e+00, 0.0000e+00, 7.5510e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.6449e+01, 7.7551e-01, 0.0000e+00, 1.7645e+02, 0.0000e+00, 7.7551e-01,\n",
      "         7.7551e-01, 0.0000e+00, 0.0000e+00, 7.7551e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.7408e+01, 7.9592e-01, 0.0000e+00, 1.8109e+02, 0.0000e+00, 7.9592e-01,\n",
      "         7.9592e-01, 0.0000e+00, 0.0000e+00, 7.9592e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.8367e+01, 8.1633e-01, 0.0000e+00, 1.8573e+02, 0.0000e+00, 8.1633e-01,\n",
      "         8.1633e-01, 0.0000e+00, 0.0000e+00, 8.1633e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [3.9327e+01, 8.3673e-01, 0.0000e+00, 1.9038e+02, 0.0000e+00, 8.3673e-01,\n",
      "         8.3673e-01, 0.0000e+00, 0.0000e+00, 8.3673e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.0286e+01, 8.5714e-01, 0.0000e+00, 1.9502e+02, 0.0000e+00, 8.5714e-01,\n",
      "         8.5714e-01, 0.0000e+00, 0.0000e+00, 8.5714e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.1245e+01, 8.7755e-01, 0.0000e+00, 1.9966e+02, 0.0000e+00, 8.7755e-01,\n",
      "         8.7755e-01, 0.0000e+00, 0.0000e+00, 8.7755e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.2204e+01, 8.9796e-01, 0.0000e+00, 2.0431e+02, 0.0000e+00, 8.9796e-01,\n",
      "         8.9796e-01, 0.0000e+00, 0.0000e+00, 8.9796e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.3163e+01, 9.1837e-01, 0.0000e+00, 2.0895e+02, 0.0000e+00, 9.1837e-01,\n",
      "         9.1837e-01, 0.0000e+00, 0.0000e+00, 9.1837e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.4122e+01, 9.3878e-01, 0.0000e+00, 2.1359e+02, 0.0000e+00, 9.3878e-01,\n",
      "         9.3878e-01, 0.0000e+00, 0.0000e+00, 9.3878e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.5082e+01, 9.5918e-01, 0.0000e+00, 2.1824e+02, 0.0000e+00, 9.5918e-01,\n",
      "         9.5918e-01, 0.0000e+00, 0.0000e+00, 9.5918e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.6041e+01, 9.7959e-01, 0.0000e+00, 2.2288e+02, 0.0000e+00, 9.7959e-01,\n",
      "         9.7959e-01, 0.0000e+00, 0.0000e+00, 9.7959e-01, 0.0000e+00, 0.0000e+00],\n",
      "        [4.7000e+01, 1.0000e+00, 0.0000e+00, 2.2752e+02, 0.0000e+00, 1.0000e+00,\n",
      "         1.0000e+00, 0.0000e+00, 0.0000e+00, 1.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
      "       requires_grad=True),) 1 and torch.Size([50, 12])\n",
      "grads shape is: torch.Size([50, 12])\n"
     ]
    }
   ],
   "source": [
    "preds, grads = preds_and_grads(test_input_tensor[idx:idx+1], net, n_steps=n_steps, target=1, numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 47.0000,   1.0000,   0.0000, 227.5250,   0.0000,   1.0000,   1.0000,\n",
       "           0.0000,   0.0000,   1.0000,   0.0000,   0.0000]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input_tensor[idx:idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('general_nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d615625bfce5ffa06059d69800a6f3c71851d453f536ef9a662c6494748581ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
