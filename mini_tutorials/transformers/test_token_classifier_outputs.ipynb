{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification \n",
    "from models import RobertaForTokenClassification, MeanRobertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing MeanRobertaForSequenceClassification: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing MeanRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing MeanRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of MeanRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForTokenClassification.from_pretrained(\"roberta-base\")\n",
    "seq_model = MeanRobertaForSequenceClassification.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs are: BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.0583,  0.0658, -0.0329,  ..., -0.0519, -0.0692, -0.0017],\n",
      "         [ 0.1603,  0.1393, -0.0063,  ...,  0.3695, -0.1458,  0.1275],\n",
      "         [ 0.2237,  0.1394,  0.1266,  ..., -0.1773, -0.0596,  0.1509],\n",
      "         ...,\n",
      "         [-0.0317,  0.0501,  0.1322,  ..., -0.0249, -0.2542,  0.0774],\n",
      "         [-0.0871, -0.1530,  0.0140,  ...,  0.1765, -0.1863,  0.2736],\n",
      "         [-0.0535,  0.0592, -0.0587,  ..., -0.0928, -0.0680, -0.0253]],\n",
      "\n",
      "        [[-0.0557,  0.0857, -0.0264,  ..., -0.0760, -0.0757, -0.0243],\n",
      "         [-0.1111, -0.1342, -0.0781,  ..., -0.1758,  0.0368, -0.0319],\n",
      "         [ 0.0406,  0.0795, -0.0182,  ..., -0.4414, -0.1000, -0.1966],\n",
      "         ...,\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017],\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017],\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=None, hidden_states=(tensor([[[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
      "         [-0.1311,  0.4242,  0.3981,  ..., -0.0402, -0.2443,  0.2164],\n",
      "         [ 0.0956,  0.2541,  0.0636,  ..., -0.0035, -0.1861, -0.1094],\n",
      "         ...,\n",
      "         [-0.0161,  0.2749,  0.5108,  ...,  0.3763,  0.0518,  0.4716],\n",
      "         [-0.3748, -0.4187,  0.3297,  ...,  0.5096, -0.6710, -0.0110],\n",
      "         [ 0.0731,  0.0143,  0.0267,  ...,  0.4326,  0.0403, -0.0697]],\n",
      "\n",
      "        [[ 0.1664, -0.0541, -0.0014,  ..., -0.0811,  0.0794,  0.0155],\n",
      "         [-0.2563,  0.1140,  0.1628,  ...,  0.0831,  0.1223, -0.4179],\n",
      "         [ 0.1608, -0.2003, -0.3351,  ..., -0.8031,  0.1610, -0.5168],\n",
      "         ...,\n",
      "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
      "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947],\n",
      "         [ 0.1619,  0.1774, -0.2718,  ..., -0.0732,  0.1385, -0.3947]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0321, -0.0055,  0.0454,  ...,  0.0374, -0.0130, -0.1207],\n",
      "         [-0.2434,  0.1920,  0.7483,  ...,  0.1100, -0.3154,  0.7078],\n",
      "         [ 0.2176,  0.1821, -0.0495,  ..., -0.1544, -0.6902, -0.0114],\n",
      "         ...,\n",
      "         [-0.4306,  0.5134,  0.8480,  ...,  0.6881,  0.2081,  0.8306],\n",
      "         [ 0.0445, -0.5729,  0.6470,  ...,  0.9956, -0.8044, -0.3654],\n",
      "         [-0.1923,  0.1217,  0.2182,  ...,  0.5551,  0.0972, -0.4789]],\n",
      "\n",
      "        [[-0.0817, -0.0164,  0.0502,  ...,  0.0122,  0.0442, -0.1003],\n",
      "         [-0.4702, -0.1506,  0.2203,  ..., -0.0342, -0.4505, -0.5446],\n",
      "         [-0.9845, -0.8065, -0.4941,  ..., -0.9504,  0.1954, -0.9389],\n",
      "         ...,\n",
      "         [-0.4452, -0.0388, -0.2049,  ...,  0.0360, -0.0689,  0.3976],\n",
      "         [-0.4452, -0.0388, -0.2049,  ...,  0.0360, -0.0689,  0.3976],\n",
      "         [-0.4452, -0.0388, -0.2049,  ...,  0.0360, -0.0689,  0.3976]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0427,  0.0072,  0.0033,  ...,  0.0338, -0.0160, -0.0661],\n",
      "         [ 0.0362,  0.4014,  0.4697,  ...,  0.2750, -0.1591,  0.7359],\n",
      "         [ 0.0154,  0.7836, -0.0357,  ..., -0.4059, -0.5913,  0.3827],\n",
      "         ...,\n",
      "         [-0.1717,  0.6836,  0.8760,  ...,  1.0852, -0.1020,  0.2676],\n",
      "         [ 0.3881,  0.1915,  0.2696,  ...,  1.2868, -1.1976, -0.4345],\n",
      "         [-0.2003,  0.2792,  0.0168,  ...,  0.9267,  0.1692, -0.5437]],\n",
      "\n",
      "        [[ 0.0435,  0.0205,  0.0031,  ...,  0.0468, -0.0106, -0.0620],\n",
      "         [-0.1717, -0.6051,  0.1702,  ..., -0.3014, -0.3731, -0.0338],\n",
      "         [-0.2916, -0.9721, -0.6555,  ..., -0.9487,  0.0876, -0.7324],\n",
      "         ...,\n",
      "         [-0.1189, -0.0330, -0.0518,  ...,  0.0858, -0.3311,  0.4180],\n",
      "         [-0.1189, -0.0330, -0.0518,  ...,  0.0858, -0.3311,  0.4180],\n",
      "         [-0.1189, -0.0330, -0.0518,  ...,  0.0858, -0.3311,  0.4180]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0217,  0.0265, -0.0127,  ...,  0.0364,  0.0519,  0.0375],\n",
      "         [-0.0894,  0.4489,  0.2788,  ...,  0.2829, -0.3095,  0.4695],\n",
      "         [ 0.0548,  0.6676, -0.1155,  ..., -0.3387, -0.6179,  0.0500],\n",
      "         ...,\n",
      "         [-0.0333,  0.0833,  0.5872,  ...,  0.9277, -0.2135,  0.1482],\n",
      "         [ 0.5929, -0.4023,  0.4247,  ...,  0.6793, -0.9056, -0.3849],\n",
      "         [-0.1971,  0.2607,  0.0322,  ...,  0.9321,  0.0471, -0.3866]],\n",
      "\n",
      "        [[ 0.0258,  0.0205, -0.0345,  ...,  0.0511,  0.0421,  0.0363],\n",
      "         [-0.0941, -0.5146,  0.4584,  ..., -0.3415, -0.6230, -0.1118],\n",
      "         [-0.1112, -0.3018, -0.6896,  ..., -0.7781, -0.1746, -0.7844],\n",
      "         ...,\n",
      "         [-0.0452, -0.1933, -0.1455,  ...,  0.0386, -0.4071,  0.3882],\n",
      "         [-0.0452, -0.1933, -0.1455,  ...,  0.0386, -0.4071,  0.3882],\n",
      "         [-0.0452, -0.1933, -0.1455,  ...,  0.0386, -0.4071,  0.3882]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0169,  0.0540,  0.0014,  ...,  0.0245, -0.0262, -0.0268],\n",
      "         [-0.3227,  0.5154,  0.3448,  ...,  0.3607, -0.4854,  0.3246],\n",
      "         [-0.1718,  0.8047,  0.1654,  ..., -0.1172, -0.8476,  0.1868],\n",
      "         ...,\n",
      "         [-0.6836, -0.3595,  0.3012,  ...,  0.9357, -0.4845,  0.2477],\n",
      "         [-0.1600, -0.2728,  0.5029,  ...,  0.7293, -1.1060, -0.3057],\n",
      "         [ 0.2641,  0.0469,  0.2335,  ...,  0.2728,  0.2198, -0.1636]],\n",
      "\n",
      "        [[-0.0077,  0.0555, -0.0080,  ...,  0.0483, -0.0180, -0.0423],\n",
      "         [-0.6015, -0.5560,  0.3370,  ..., -0.1565, -0.9015, -0.2523],\n",
      "         [-0.4076, -0.0768, -0.4843,  ..., -0.2732, -0.4616, -0.6099],\n",
      "         ...,\n",
      "         [-0.0360, -0.2853,  0.1979,  ...,  0.1623,  0.2416,  0.0546],\n",
      "         [-0.0360, -0.2853,  0.1979,  ...,  0.1623,  0.2416,  0.0546],\n",
      "         [-0.0360, -0.2853,  0.1979,  ...,  0.1623,  0.2416,  0.0546]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 2.0137e-02,  7.1162e-02,  2.7657e-02,  ..., -7.0634e-03,\n",
      "           3.2986e-02,  2.2552e-02],\n",
      "         [-1.9102e-01,  5.9018e-01,  4.8125e-01,  ...,  2.1566e-01,\n",
      "           1.3696e-01, -1.6340e-01],\n",
      "         [-1.1066e-02,  7.5852e-01, -3.3496e-02,  ...,  3.9188e-02,\n",
      "           9.6084e-03, -3.9281e-01],\n",
      "         ...,\n",
      "         [-2.7446e-01, -2.4713e-01,  6.0888e-01,  ...,  5.9221e-01,\n",
      "          -4.7169e-01,  3.4442e-01],\n",
      "         [ 2.2351e-01, -3.3923e-01,  2.7781e-01,  ...,  4.2566e-01,\n",
      "          -7.4680e-01, -6.7017e-02],\n",
      "         [ 6.2376e-02, -3.4035e-02,  6.0809e-02,  ...,  1.1926e-01,\n",
      "          -1.8549e-02,  7.6703e-03]],\n",
      "\n",
      "        [[ 5.8271e-03,  7.5679e-02,  2.5114e-02,  ...,  2.4800e-02,\n",
      "          -5.4691e-04,  1.8474e-02],\n",
      "         [-2.6620e-01, -4.9821e-01,  7.7566e-01,  ..., -1.9178e-01,\n",
      "          -7.9269e-01, -7.3615e-01],\n",
      "         [-1.9803e-01, -1.8159e-01, -4.3863e-01,  ..., -2.6345e-01,\n",
      "          -4.1074e-01, -8.9666e-01],\n",
      "         ...,\n",
      "         [-3.9100e-01, -1.9662e-01,  1.7299e-01,  ...,  3.5678e-01,\n",
      "           6.5559e-01, -3.7072e-01],\n",
      "         [-3.9100e-01, -1.9662e-01,  1.7299e-01,  ...,  3.5678e-01,\n",
      "           6.5559e-01, -3.7072e-01],\n",
      "         [-3.9100e-01, -1.9662e-01,  1.7299e-01,  ...,  3.5678e-01,\n",
      "           6.5559e-01, -3.7072e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 8.7999e-02,  5.2284e-02,  1.2090e-01,  ...,  9.0157e-03,\n",
      "          -3.0837e-02, -4.1063e-02],\n",
      "         [ 2.4092e-01,  4.3682e-01,  3.7731e-01,  ...,  3.1788e-01,\n",
      "          -2.0684e-01,  4.2009e-02],\n",
      "         [ 3.3308e-01,  6.4918e-01, -1.2530e-01,  ...,  1.9574e-02,\n",
      "          -1.6517e-01,  5.9144e-03],\n",
      "         ...,\n",
      "         [ 1.9701e-01, -2.8233e-01,  3.2465e-01,  ...,  6.3698e-01,\n",
      "          -3.3083e-01,  4.7082e-01],\n",
      "         [ 4.1934e-01, -3.6210e-01,  3.4087e-01,  ...,  6.9877e-01,\n",
      "          -7.5132e-01,  7.1206e-03],\n",
      "         [ 3.2305e-02,  1.8603e-03,  9.4602e-02,  ...,  7.2050e-04,\n",
      "          -5.7452e-02, -2.6341e-02]],\n",
      "\n",
      "        [[ 7.6427e-02,  5.9921e-02,  1.1051e-01,  ...,  2.6295e-02,\n",
      "          -3.0887e-02, -5.8541e-02],\n",
      "         [-9.4912e-02, -5.0058e-01,  6.3774e-01,  ..., -2.6019e-01,\n",
      "          -7.7035e-01, -5.7151e-01],\n",
      "         [-2.3308e-01, -1.9302e-01, -5.9869e-01,  ..., -3.0373e-01,\n",
      "          -3.0484e-01, -8.5706e-01],\n",
      "         ...,\n",
      "         [-3.6033e-02, -2.5260e-01,  7.0062e-01,  ...,  4.8571e-01,\n",
      "           7.0490e-01,  5.9035e-02],\n",
      "         [-3.6033e-02, -2.5260e-01,  7.0062e-01,  ...,  4.8571e-01,\n",
      "           7.0490e-01,  5.9035e-02],\n",
      "         [-3.6033e-02, -2.5260e-01,  7.0062e-01,  ...,  4.8571e-01,\n",
      "           7.0490e-01,  5.9035e-02]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0153,  0.0467,  0.0616,  ...,  0.1220, -0.0390,  0.0058],\n",
      "         [ 0.0864,  0.4338,  0.1935,  ...,  0.6547,  0.0769,  0.2295],\n",
      "         [ 0.2248,  0.5282, -0.1079,  ...,  0.1569, -0.1185,  0.3449],\n",
      "         ...,\n",
      "         [-0.2249, -0.3167,  0.0858,  ...,  0.4520, -0.5061,  0.3998],\n",
      "         [ 0.0504, -0.5494,  0.4695,  ...,  0.7483, -0.8330,  0.2002],\n",
      "         [ 0.0049,  0.0082,  0.0694,  ...,  0.0183,  0.0018,  0.0272]],\n",
      "\n",
      "        [[-0.0279,  0.0740,  0.0603,  ...,  0.1303, -0.0433,  0.0092],\n",
      "         [-0.0313, -0.5386,  0.3799,  ..., -0.2930, -0.5879, -0.3798],\n",
      "         [ 0.0227,  0.0752, -0.5729,  ..., -0.3200, -0.2635, -0.5951],\n",
      "         ...,\n",
      "         [-0.1654, -0.7498,  0.8412,  ...,  0.4859,  0.5119,  0.3218],\n",
      "         [-0.1654, -0.7498,  0.8412,  ...,  0.4859,  0.5119,  0.3218],\n",
      "         [-0.1654, -0.7498,  0.8412,  ...,  0.4859,  0.5119,  0.3218]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.0784e-01,  1.4504e-01,  8.4944e-03,  ...,  1.0737e-01,\n",
      "           1.5029e-02, -2.4560e-02],\n",
      "         [ 1.5147e-01,  4.5900e-01,  2.5054e-01,  ...,  6.2207e-01,\n",
      "           1.5453e-01,  1.9782e-01],\n",
      "         [ 3.9250e-01,  8.7255e-01, -4.3056e-02,  ..., -1.4342e-01,\n",
      "           1.0522e-01,  1.0705e-01],\n",
      "         ...,\n",
      "         [-1.6412e-01, -3.6627e-01, -5.0917e-02,  ...,  3.3812e-01,\n",
      "          -1.2701e-01,  4.1969e-01],\n",
      "         [-4.8130e-01, -5.6760e-01,  2.0605e-01,  ...,  9.1360e-01,\n",
      "          -6.6527e-01,  8.3584e-02],\n",
      "         [-4.2000e-03,  3.9930e-02,  2.7301e-02,  ...,  6.8294e-03,\n",
      "          -1.0938e-04, -1.0264e-02]],\n",
      "\n",
      "        [[-1.3274e-01,  1.4249e-01,  2.9638e-02,  ...,  1.2352e-01,\n",
      "           1.7234e-02, -3.9529e-02],\n",
      "         [-7.0549e-02, -7.7310e-01,  5.1505e-01,  ..., -1.5414e-01,\n",
      "          -6.2057e-01, -1.2630e-01],\n",
      "         [ 2.1419e-01, -1.1107e-01, -2.8012e-01,  ..., -3.2259e-01,\n",
      "          -1.1274e-01, -4.2030e-01],\n",
      "         ...,\n",
      "         [-3.2033e-01, -8.3854e-01,  6.7564e-01,  ...,  6.6172e-01,\n",
      "           4.2577e-01,  1.1102e-01],\n",
      "         [-3.2033e-01, -8.3854e-01,  6.7564e-01,  ...,  6.6172e-01,\n",
      "           4.2577e-01,  1.1102e-01],\n",
      "         [-3.2033e-01, -8.3854e-01,  6.7564e-01,  ...,  6.6172e-01,\n",
      "           4.2577e-01,  1.1102e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0905,  0.0426, -0.1075,  ...,  0.1192,  0.0536, -0.0125],\n",
      "         [-0.0711,  0.2741,  0.5405,  ...,  0.5000, -0.0401,  0.1750],\n",
      "         [-0.0502,  0.8612, -0.0341,  ..., -0.0996, -0.0136,  0.0993],\n",
      "         ...,\n",
      "         [-0.1135, -0.1284, -0.1793,  ...,  0.1548,  0.1085,  0.5816],\n",
      "         [-0.3934, -0.7849,  0.1099,  ...,  0.7320, -0.2100,  0.6536],\n",
      "         [ 0.0300,  0.0587,  0.1337,  ...,  0.0139,  0.1943, -0.1355]],\n",
      "\n",
      "        [[-0.1077,  0.0343, -0.0936,  ...,  0.1203,  0.0498, -0.0206],\n",
      "         [-0.2230, -0.4881,  0.0521,  ..., -0.2953, -0.4490,  0.1941],\n",
      "         [ 0.4918, -0.0877, -0.4284,  ..., -0.6432, -0.2442, -0.3941],\n",
      "         ...,\n",
      "         [ 0.0458, -0.9847,  0.6764,  ...,  0.5033,  0.2280,  0.0550],\n",
      "         [ 0.0458, -0.9847,  0.6764,  ...,  0.5033,  0.2280,  0.0550],\n",
      "         [ 0.0458, -0.9847,  0.6764,  ...,  0.5033,  0.2280,  0.0550]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0093,  0.0415, -0.0927,  ...,  0.0891,  0.0851,  0.0072],\n",
      "         [ 0.3865,  0.3543,  0.3547,  ...,  0.5989, -0.1799,  0.0528],\n",
      "         [ 0.3296,  0.6970,  0.0925,  ..., -0.3457, -0.1207,  0.0237],\n",
      "         ...,\n",
      "         [ 0.2892, -0.1808, -0.2331,  ...,  0.1711, -0.3052,  0.3243],\n",
      "         [-0.5922, -0.5397,  0.0149,  ...,  0.7755,  0.0874,  0.3436],\n",
      "         [-0.0940,  0.0470,  0.0305,  ...,  0.0443,  0.0905, -0.0214]],\n",
      "\n",
      "        [[-0.0137,  0.0202, -0.0935,  ...,  0.0798,  0.0763,  0.0049],\n",
      "         [ 0.0345, -0.3248,  0.0793,  ..., -0.3410, -0.4393,  0.2434],\n",
      "         [ 0.6184,  0.1426, -0.4792,  ..., -0.6415, -0.4094, -0.4157],\n",
      "         ...,\n",
      "         [-0.0708, -0.8759,  0.6055,  ...,  0.5553, -0.0612,  0.0575],\n",
      "         [-0.0708, -0.8759,  0.6055,  ...,  0.5553, -0.0612,  0.0575],\n",
      "         [-0.0708, -0.8759,  0.6055,  ...,  0.5553, -0.0612,  0.0575]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0027, -0.0414, -0.0253,  ...,  0.0894, -0.0790,  0.1027],\n",
      "         [ 0.1446,  0.1058,  0.1116,  ...,  0.6558, -0.4125,  0.1557],\n",
      "         [ 0.3766,  0.2864,  0.1129,  ..., -0.0795, -0.1373,  0.1884],\n",
      "         ...,\n",
      "         [ 0.0323, -0.3657,  0.2384,  ...,  0.1646, -0.5975,  0.2903],\n",
      "         [-0.3592, -0.5871, -0.0572,  ...,  0.7296, -0.0792,  0.4233],\n",
      "         [-0.0272, -0.0318,  0.0209,  ...,  0.0255, -0.0017,  0.0671]],\n",
      "\n",
      "        [[-0.0162, -0.0218,  0.0008,  ...,  0.0774, -0.0886,  0.0465],\n",
      "         [-0.2324, -0.5225,  0.0727,  ..., -0.2423,  0.0407,  0.0597],\n",
      "         [ 0.4890,  0.0158, -0.1202,  ..., -0.5450, -0.1050, -0.5257],\n",
      "         ...,\n",
      "         [-0.0345, -0.4579,  0.3244,  ...,  0.3415,  0.0153,  0.1301],\n",
      "         [-0.0345, -0.4579,  0.3244,  ...,  0.3415,  0.0153,  0.1301],\n",
      "         [-0.0345, -0.4579,  0.3244,  ...,  0.3415,  0.0153,  0.1301]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0583,  0.0658, -0.0329,  ..., -0.0519, -0.0692, -0.0017],\n",
      "         [ 0.1603,  0.1393, -0.0063,  ...,  0.3695, -0.1458,  0.1275],\n",
      "         [ 0.2237,  0.1394,  0.1266,  ..., -0.1773, -0.0596,  0.1509],\n",
      "         ...,\n",
      "         [-0.0317,  0.0501,  0.1322,  ..., -0.0249, -0.2542,  0.0774],\n",
      "         [-0.0871, -0.1530,  0.0140,  ...,  0.1765, -0.1863,  0.2736],\n",
      "         [-0.0535,  0.0592, -0.0587,  ..., -0.0928, -0.0680, -0.0253]],\n",
      "\n",
      "        [[-0.0557,  0.0857, -0.0264,  ..., -0.0760, -0.0757, -0.0243],\n",
      "         [-0.1111, -0.1342, -0.0781,  ..., -0.1758,  0.0368, -0.0319],\n",
      "         [ 0.0406,  0.0795, -0.0182,  ..., -0.4414, -0.1000, -0.1966],\n",
      "         ...,\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017],\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017],\n",
      "         [-0.0327,  0.0106,  0.0424,  ...,  0.0111, -0.0658,  0.0017]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), past_key_values=None, attentions=None, cross_attentions=None)\n",
      "Outputs shape is: torch.Size([2, 12, 768])\n",
      "Sequence output shape is: torch.Size([2, 12, 768])\n",
      "Logits shape is: torch.Size([2, 12, 2])\n"
     ]
    }
   ],
   "source": [
    "# generate random input sequence, tokenize and pass to model\n",
    "input_sequence = [\"This is a test sequence rabble cheese over there\", \"random shit\"]\n",
    "input_ids = tokenizer(input_sequence, return_tensors=\"pt\", padding = True)\n",
    "output = model(**input_ids, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence output shape is: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "# and pass to sequence model\n",
    "output2 = seq_model(**input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-0.2190, -0.0640]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForTokenClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a linear layer \n",
    "\n",
    "linear_layer = torch.nn.Linear(768, 2, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_output = linear_layer(output.hidden_states[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0232,  0.0685],\n",
       "         [ 0.1119,  0.1944],\n",
       "         [ 0.1769,  0.3318],\n",
       "         [ 0.0780, -0.0310],\n",
       "         [-0.1006, -0.1217],\n",
       "         [-0.0700,  0.3111],\n",
       "         [ 0.1485,  0.3663],\n",
       "         [-0.2116,  0.3100],\n",
       "         [ 0.2791,  0.1702],\n",
       "         [ 0.2133,  0.1323],\n",
       "         [ 0.2286,  0.1328],\n",
       "         [ 0.4357,  0.1773]],\n",
       "\n",
       "        [[-0.0232,  0.0685],\n",
       "         [-0.0553,  0.2431],\n",
       "         [ 0.1930,  0.1947],\n",
       "         [ 0.3234,  0.1710],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass single batch alone\n",
    "single_linear_output = linear_layer(output.hidden_states[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0232,  0.0685],\n",
       "        [ 0.1119,  0.1944],\n",
       "        [ 0.1769,  0.3318],\n",
       "        [ 0.0780, -0.0310],\n",
       "        [-0.1006, -0.1217],\n",
       "        [-0.0700,  0.3111],\n",
       "        [ 0.1485,  0.3663],\n",
       "        [-0.2116,  0.3100],\n",
       "        [ 0.2791,  0.1702],\n",
       "        [ 0.2133,  0.1323],\n",
       "        [ 0.2286,  0.1328],\n",
       "        [ 0.4357,  0.1773]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "single_linear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decompose the linear layer by doing matmul \n",
    "linear_layer.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 12, 768])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.hidden_states[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0232,  0.0685],\n",
       "         [ 0.1119,  0.1944],\n",
       "         [ 0.1769,  0.3318],\n",
       "         [ 0.0780, -0.0310],\n",
       "         [-0.1006, -0.1217],\n",
       "         [-0.0700,  0.3111],\n",
       "         [ 0.1485,  0.3663],\n",
       "         [-0.2116,  0.3100],\n",
       "         [ 0.2791,  0.1702],\n",
       "         [ 0.2133,  0.1323],\n",
       "         [ 0.2286,  0.1328],\n",
       "         [ 0.4357,  0.1773]],\n",
       "\n",
       "        [[-0.0232,  0.0685],\n",
       "         [-0.0553,  0.2431],\n",
       "         [ 0.1930,  0.1947],\n",
       "         [ 0.3234,  0.1710],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596],\n",
       "         [-0.1982,  0.0596]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matmul hidden states with linear layer weights and add bias\n",
    "output.hidden_states[0] @ (linear_layer.weight.data).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "39nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
